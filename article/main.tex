\documentclass[a4paper, 11pt]{article}
% \usepackage{ae,lmodern}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[USenglish]{babel}
\usepackage[margin=3cm]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section] % Number within sec ?
% \newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{observation}[theorem]{Observation}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% \newtheorem*{remark*}{Remark}
% \theoremstyle{definition}
% \newtheorem{problem}{Problem}
% \newtheorem{openproblem}{Open Problem}

\newcommand{\NN}{\mathbb{N}}%
\newcommand{\ZZ}{\mathbb{Z}}%
\newcommand{\Sbb}{\mathbb{S}}%
\newcommand{\Cc}{\mathcal{C}}%
\newcommand{\Gg}{\mathcal{G}}%
\newcommand{\Ff}{\mathcal{F}}%
\newcommand{\Ss}{\mathcal{S}}%
\newcommand{\eps}{\varepsilon}%
\newcommand{\epsm}{\eps^{-1}}%
\newcommand{\bit}{\{0,1\}}%
\DeclareMathOperator*{\poly}{poly}

\newcommand{\Omegat}[1]{\widetilde{\Omega}\left( #1 \right)}%
\newcommand{\Ot}[1]{\widetilde{O}\left( #1 \right)}%

\newcommand{\Poly}{\textsf{P}}%
\newcommand{\Log}{\textsf{L}}%
\newcommand{\NP}{\textsf{NP}}%
\newcommand{\TIME}{\textsf{TIME}}%
\newcommand{\NTIME}{\textsf{NTIME}}%
\newcommand{\coNTIME}{\textsf{coNTIME}}%
\newcommand{\SAT}{\textsf{SAT}}%
\newcommand{\ParSAT}{\ensuremath\bigoplus\textsf{SAT}}%
\newcommand{\SharpSAT}{\#\textsf{SAT}}%
\newcommand{\NAND}{\textsf{NAND}}%
\newcommand{\ND}{\textsf{NDepth}}%
\newcommand{\NDL}[1]{\ND[ #1 \log n]}%

\newcommand{\todohere}{\todo[inline]{TODO}}%

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\title{Uniform formula lower bounds for testing the existence of and counting satisfiable assignments}
\author{Gabriel Bathie, Ryan Williams}
\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Motivation: understanding complexity. Explicit lower bounds : programs, circuits, formulas.
Uniformity.

A line of work initiated by Fortnow~\cite{fortnow2000time,fortnow2005time} in 1997 
has proven model-independent time-space lower bounds for the SAT
problem and related problems within the polynomial-time hierarchy.
This approach was then extended by Williams~\cite{williams2006inductive,williams2007time,williams2013alternation} 
into a complex framework which culminated in proving that \SAT{} 
does not have a small-space algorithm that runs in time $n^{2\cos(\pi/7)} \simeq n^{1.801}$.
This line of work concluded~\cite{buss2015limits} with a proof 
that the $2\cos(\pi/7)$ exponent is optimal for this framework,
which implies that radically new ideas would be required to separate \NP{} from \Log{}.
This approach was later expanded to obtain slightly stronger lower bounds
for quantum classes, and against small-space randomized computation~\cite{mudigonda2020time}.

In this work, we explore a new extension of this line of work, 
which gives lower bounds on the size uniform formulas that solve \SAT{}
and related problems, such as \ParSAT{} and \SharpSAT{}.
By giving an efficient simulation of uniform log-depth \NAND{} formulas 
by $\Sigma_2$ computation, we are able to show that \SAT{}
does not have \textit{uniform} \NAND{} formulas 
of depth less than~$4 \cos(\pi/7) \log n \simeq 3.603 \log n$.

TODO: We also extend this result to show that \SharpSAT{} does not have
alternating AND-OR/XOR-EQ formulas of depth less than~$3.603 \log n$.
\todo[inline]{does that extend to $n^c$-size lower bound for some $c > 3$?}

Finding strong lower bounds on the formula size of explicit functions in \NP{}
is a long standing open problem.
The best lower bounds known depend on the computational basis, that is, 
the set of gates that are allowed in the formula.
For the full binary basis $B_2$, Nechiporuk\cite{nechiporuk66boolean}
proved in 1966 an $\Omegat{n^2}$ lower bound for an explicit function in \NP{}, 
which remains unbeaten to this day.
For the De Morgan basis $\{\wedge, \vee, \neg\}$, Andreev~\cite{andreev1987method} 
proved an $\Omegat{n^{2.5}}$ lower bound, 
which was later improved by H{\aa}stad~\cite{hastad1998shrinkage} to $\Omegat{n^{3}}$.
This lower bound is optimal up to polylog factors,
since their function has (uniform) formulas of size $\Ot{n^{3}}$.
% Another interesting case is that of mononotone formulas, i.e. formulas over the $\{\wedge, \vee\}$ basis,
% for which Razborov, Alon and Boppana~\cite{razborov1985lower, alon1987monotone} showed
% a superpolynomial $n^{\Theta(\log n)}$ lower bounds for the \NP-complete \textsf{Clique} problem.
\todo{add monotone circuits lower bounds? But that's not formulas}

Note that all the lower bounds mentioned in the previous paragraph 
are \textit{size} lower bounds of the form $n^c$,
and they hold for \textit{non-uniform} formulas.  
The lower bounds that we give are \textit{depth} lower bounds of the form $c\log n$,
and only hold for \textit{uniform} formulas.
While a $n^c$ size lower bound implies a $c\log n$ depth lower bounds,
the converse is not immediately true; 
one may get an $n^c$ size lower bound from an
$\alpha c\log n$ depth lower bound, where $\alpha$ depends on the computation basis,
using the work of Brent~\cite{brent1974parallel} and Spira~\cite{spira1971time}.
\todo{can we show something here? e.g. improve for $B_2$?}

%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results overview}

We first study \NAND{} formulas. 
The \NAND{} basis is rather expressive: Andreev's function has \NAND{} formulas of depth $(3+o(1)) \log n$ (and therefore of size $\Ot{n^3}$), which matches the lower bound for the De Morgan Basis.


Namely, we prove the following:
\begin{theorem}
	For any $c < 4 \cos(\pi/7) \simeq 3.603$, \SAT{} does not have \NAND{} formulas of depth $(c + o(1)) \log n$.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Definitions}

\subsection{Uniform formulas}
A family of formulas

In this work, we focus on formulas with depth $d(n) = (c + o(1))\log n$, for some $c > 1$.

\begin{definition}[``Usual'' polylog-time uniform formulas]
	A family of formulas $(\varphi_n)_{n \ge 1}$ of depth $(c + o(1))\log n$ 
	is \textit{polylog-time uniform} if there exists a machine $A_\varphi$
	such that for every $n$ and every $b\in\bit^{\leq (c + o(1))\log n}$,
	$A_\varphi(b, n)$ outputs a description of the gate at position $b$ in $\varphi_n$ in time $\log^{O(1)} n$.
\end{definition}

Here, the position of a gate is given by a bitstring of length at most $(c + o(1))\log n$ 
that describes the path to follow to reach that gate from the root, 
with $0$ meaning ``go to left child'' and $1$ ``go to right child''.


In this work, we consider a stronger model of uniform formulas, 
where the descriptor machine $A_\varphi$ has random access to the input $x$ 
at which we wish to evaluate the formula, but is still limited to polylogarithmic time.
\begin{definition}[Input-aware polylog-time uniform formulas]\todo{is this a good name?}\label{def:unif}
	A family of formulas $(\varphi_n)_{n \ge 1}$ of depth $(c + o(1))\log n$ is \textit{polylog-time uniform}
	if there exists a \textbf{RAM} machine $B_\varphi$
	such that for every $n$, every $x\in\bit^n$ and every 
	$b\in\bit^{\leq (c + o(1))\log n}$,
	$B_\varphi(x, b, n)$ outputs a description of the gate at position $b$ in $\varphi_n$ in time $\log^{O(1)} n$.
\end{definition}
Notice that this model supersedes the previous one: 
$A_\varphi$ satisfies the conditions of Definition~\ref{def:unif}, 
but does not use its random access to the input, 
therefore the lower bounds that we give against model also hold for the usual one.

Moreover, this definition allows for an efficient simulation of uniform formulas by RAM machines, 
due to the negligible overhead.
Indeed, for every $c > 1$, we have $\ND[c \log n] \subseteq TS[n^c]$.

We will see the need for this stronger definition in \todo{ref}, 
where we need to give $O(\log n)$ bits of information to the descriptor
to adaptively choose a subformula.

Uniform \NAND{}/alternating formulas. Specific model, comparison with the usual model.

\subsection{Quantified/Alternating computation}

We now introduce ``quantified'' complexity classes, which extend alternating classes 
by adding a ``parity'' quantifier.

Given a complexity class $\Cc$ and constants $k, (a_i)_{i\leq k}$, 
we consider \textit{quantified} complexity
class $(Q_1~n^{a_1})\ldots(Q_k~n^{a_k})~\Cc$, 
where $Q_i$ is one of $\forall$ or $\exists$
 % or $\oplus$.
A computation in a quantified complexity class is defined as follows:
\begin{definition}[Quantified computation]
	A language $L$ is in $(Q_1~n^{a_1})\ldots(Q_k~n^{a_k})~\Cc$ 
	is defined by $k$ RAM machines $A_1, \ldots, A_k$ and a $\Cc$ machine $M$
	such that, given an input $x$ of length $n$:
	for every $i \le k$, $A_{i}$ takes as input $x_i$ along with $y_i$, 
	a string of $n^{a_{i} + o(1)}$ nondeterministic bits,
	runs in time $n^{a_{i} + o(1)}$ and outputs a string $x_{i}$ of length at most $n^{a_{i} + o(1)}$, 
	with the convention that $x_0 := x$.
	The output of the last stage, $x_{k}$, is passed to the machine $M$. 

	The computation accepts $x$ \textit{starting from stage $i$} if either:
	\begin{itemize}
		\item $i = k+1$ and $M$ accepts $x_{k}$, 
		\item $Q_i$ is $\exists$, and there exists a $y_i$ such that the computation
		accepts $x_{i}$ starting from stage $i+1$, 
		\item $Q_i$ is $\forall$, and for every $y_i$, the computation
		accepts $x_{i}$ starting from stage $i+1$. 
		% \item $Q_i$ is $\bigoplus$, the number of $y_i$ such that the computation
		% accepts $x_{i}$ starting from stage $i+1$ is odd. 
	\end{itemize}

	An input $x$ is in $L$ if the computation $x$ accepts starting from stage 1.
\end{definition}
\todo[inline]{allow $O(\log n)$ quantifiers ?}

\subsection{Overview of the methodology}

The general methodology used in this paper to obtain lower bounds
extends the one used in~\cite{mudigonda2020time}.

To prove that \SAT{} does not have formulas of depth less than $c\log n$, 
we reason by contradiction.
First, we assume that \SAT{} has such formulas, and the strong \NP-completeness of \SAT{}
allows us to deduce that the same is holds for all of $\NTIME[n]$, 
\begin{lemma}\label{lemma:usual_sd}
	If \SAT{} is in $\NDL{c}$ for some $c>0$,
	then $\NTIME[n] \cup \coNTIME[n] \subseteq \NDL{c}$.
\end{lemma}

We also show how to speed-up (or rather, reduce the depth of) computations by adding quantifiers, 
using various ``Speedup rules''. 
For example, for NAND formulas (which are the same as alternating AND/OR formulas), we have the following:
\begin{lemma}[Speedup rule]
	For every $c > 0, x \leq c$, we have:
	\[\NDL{c} \subseteq (\exists n^{x/2}) (\forall n^1) \NDL{(c-x)}\]
\end{lemma}


The above lemma can be combined with exhaustive search and Lemma~\ref{lemma:usual_sd} 
to yield an improved version of Lemma~\ref{lemma:usual_sd} for quantified classes, 
which we call the ``Slowdown rule''.
\begin{lemma}[Slowdown rule]
	Assume that \SAT{} is in $\NDL{c}$ for some $c>0$.
	Then, for every $a,b,d > 0$, we have:
	\[\ldots (Q~n^{a}) (\neg Q~n^b) \NDL{d}
	\subseteq \ldots (Q~n^{a}) \NDL{c\cdot\max(d/2, a, b)}.\]
	% \[\ldots (Q_{k-1}~n^{a_{k-1}}) (Q_k~n^{a_k}) \NDL{d} 
	% \subseteq (Q_{k-1}~n^{a_{k-1}}) \NDL{c\cdot \max(d/2, a_k, a_{k-1})}\]	
\end{lemma}


To prove our results, 
we combine multiple applications of these rules with suitable parameters,
with the goal of contradicting a hierarchy theorem.
In this work, we aim to reach a contradiction of the following hierarchy
theorem for NAND formulas, that we obtain from the assumption that
$\NTIME[n] \subseteq \NDL[c]$ for some $c$, 
as a corollary of the Nondeterministic Time Hierarchy Theorem:
\begin{theorem}\label{thm:nandh}
	If there exists $c > 0$ such that $\NTIME[n] \subseteq \NDL{c}$, 
	then for all $b > a > 0$, we have
	\[\NDL{a} \subsetneq \NDL{b}.\] 
\end{theorem}

For example, we can prove that SAT does not have NAND formulas 
of depth less than $2.8284 \log n$ with the following proof.
Assume that $\SAT\in \textsf{NAND-depth}[c \log n]$ for some $c > 0$.
Then, we have
\begin{flalign*}
	\NDL{4}
		&\subseteq (\exists n)(\forall n)\NDL{2}& \text{Speedup rule with param. } x = 2\\
		&\subseteq (\exists n)\NDL{c}	& \text{Slowdown rule}\\
		&\subseteq \NDL{c^2/2}	& \text{Slowdown rule}
\end{flalign*}
For $c^2/2  < 4 \Leftrightarrow c < 2\sqrt{2} \simeq 2.8284\ldots$, this contradicts Theorem~\ref{thm:nandh}.

In practice, we use (integer) linear programming to find the
best sequence of rules to apply, along with the optimal parameters.
See Appendix~\ref{app:ilp} for a detailed presentation of the models that we use.

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of main result}

\subsection{Speedup and Slowdowns}
We start by discussion the Speedup rule for Alternating formulas.

\begin{lemma}[Speedup rule]\label{lemma:speedup}
	For every $d > 0$ and $0 \ge x \ge d$, we have
	\begin{flalign*}
	\NDL{d} &\subseteq (\exists n^{x/2}) (\forall \frac{x}{2}\log n) \NDL{d-x}\text{ and}\\
	\NDL{d} &\subseteq (\forall n^{x/2}) (\exists \frac{x}{2}\log n) \NDL{d-x}
	\end{flalign*}
\end{lemma}
\todo[inline]{fix quantifiers?}

The intuitive idea behind this lemma is that the only
information needed to know that an OR gate evaluates
to 1 is that one of its children evaluates to 1. 
We apply this idea to every OR gate in the first $x \log n$
levels of the formula,
and then use a universal quantifier to handle the remaining AND gates.

The role of the existential quantifier is to guess for every OR gate in the first $x\log n$ levels,
which of its children evaluates to 1, and we only evaluate the corresponding subformula.
Moreover, making this guess reduces the number of OR gates that we will evaluate
(since the formula alternate between levels of OR and AND gates, this number doubles every two levels, instead of doubling at every level), 
therefore we only need to guess $O(n^{x/2})$ bits. 

We can then eliminate OR gates and replace them with their selected child.
We are left with a formula starting with $(x/2)\log n$ levels of AND gates, 
followed by smaller OR/AND formulas of depth $(d-x + o(1)) \log n$.
The main formula evaluate to 1 if and only if for every of these $O(n^{x/2})$
smaller formulas evaluates to true: 
we can check this using a universal quantifiers, 
which requires $O(\frac{x}{2}\log n)$ bits, and then running an $\NDL{d-x}$ computation.

Finally, notice that the definition of alternating computation
requires that there exists a \textit{single} machine $M$ that runs the
final stage of computation \textit{for every} possible choice of nondeterministic bits.
This is where we use the strong notion of uniformity (from Definition~\ref{def:unif}), where the descriptor machine
has random access to the input: giving $M$ access to (a few) bits of the input
allows us to adapt its behavior depending on the nondeterministic bits. 

\begin{proof}[Proof of Lemma~\ref{lemma:speedup}]
	We prove the result for $\exists\forall$ computations, 
	the result for $\forall\exists$ computations follows from the fact
	that $\NDL{d}$ is closed under complementation.
	
	Let $L\in\NDL{d}$, let $M : (x,i,n) \mapsto g$ be 
	a descriptor machine for $L$, and let $x$ be an input of length $n$.
	
	We now construct a machine $M'$ that describes $\NDL{d-x}$-formulas,
	such that an $\exists\forall$ computation with $M'$ as its last stage accepts $L$.
	On an input $x$

	We describe how to build bit strings $y$ of length $n^{x/2}$ and
	\todohere 
\end{proof}


\begin{corollary}\label{cor:speedup}
	For every $d > 0$, we have
	\[\NDL{d} \subseteq \NTIME[n^{d/2}] \cap \coNTIME[n^{d/2}].\]
\end{corollary}
\begin{proof}
	Applying Lemma~\ref{lemma:speedup} with $x = d$, we obtain
	$\NDL{d} \subseteq (\exists n^{d/2}) (\forall \frac{d}{2}\log n) \NDL{0}$.
	Now, using an exhaustive search over the $n^{d/2}$ bit strings of length $\frac{d}{2}\log n$
	followed by a deterministic $n^{o(1)}$ computation, we get that
	$\NDL{d} \subseteq (\exists n^{d/2}) \TIME[n^{d/2}] = \NTIME[n^{d/2}]$.

	The inclusion in $\coNTIME[n^{d/2}]$ follows from the stability of $\NDL{d}$
	under complement.
\end{proof}


We now discuss how to efficiently remove quantifiers from an alternating \ND{} computation.
Slowdown rule from algorithms for SAT assumptions:
\begin{lemma}[Slowdown rule]\label{lemma:slowdown}
	If $\SAT{}\in \NDL{c}$, then for every $d \ge 1$, $\NTIME[n^d] \cup \coNTIME[n^d] \subseteq \NDL{c\cdot d}$.
\end{lemma}

A key idea for the proof of Lemma~\ref{lemma:slowdown} 
is the fact that any language in $\NTIME[n]$ can
be reduced to $\SAT{}$ via uniform poly-log time first-order projections, 
with instances of size $n$ reduced to formulas of size $n^{1+o(1)}$.
Intuitively, a first order projection is a depth 0 circuit, 
i.e. each output bit is either a constant (0 or 1),
an input $x_i$ or its negation, $\neg x_i$.
For a more in-depth exposition of first-order projections,
see for example~\cite[End of Sec.~3]{allender1997first}.
\begin{proof}[Proof of Lemma~\ref{lemma:slowdown}]
	We first prove that $\SAT{}\in \NDL{c}$ implies that $\NTIME[n] \subseteq \NDL{c}$,
	then use a padding argument to show that $\forall d\ge 1, \NTIME[n^d] \subseteq \NDL{c\cdot d}$,
	and the result follows since $\NDL{c\cdot d}$ is closed under complement.

	Let $L\in\NTIME[n]$. 
	As stated above, $L$ is Karp-reducible to \SAT{} via
	uniform poly-log time first-order projections, 
	\todo{ref for this? Folklore?}
	with instances of size $n$ reduced to formulas of size $n^{1+o(1)}$.
	Now, notice that the composition of a NAND formula of depth $d$ with a first order projection
	is also a NAND formula of depth $d$: it only consists in relabeling leaves of the formula.
	Poly-log time uniformity is also preserved. 
	Therefore, we can use $(c+o(1)) \log n$-depth NAND formulas for \SAT{} on $n^{1+o(1)}$
	inputs to decide $L$ by composing them with the first order projection; 
	the resulting formula has size $(c+o(1)) \log n^{1+o(1)} = (c+o(1)) \log n$,
	therefore $L\in\NDL{c}$.

	We now use a padding argument to lift this result to $\NTIME[n^d]$.
	Let $d>1$, let $L\in \NTIME[n^d]$ and let $M$ be a nondeterministic machine that decides $L$
	in time $n^{d+o(1)}$.
	Consider the padded language $L' = \{x1^{|x|^d - |x|} \mid x\in L\}$.
	Given an input $y$ of length $m$, we can decide whether $y\in L'$ in time 
	$m^{1+o(1)}$ as follows:
	\begin{itemize}
		\item if $y$ is not of the form $x1^{|x|^d - |x|}$ for some $x$, reject $y$.
		This can be done in time $O(m)$.
		\item otherwise, $y = x1^{|x|^d - |x|}$. Run $M$ on $x$, and accept $y$ 
		if and only it accepts $x$. This takes time $|x|^{d+o(1)} = m^{1+o(1)}$.
	\end{itemize}
	This implies $L'$ is in $\NTIME[n]$, which in turn implies that $L'\in\NDL{c}$.
	We can then use the formula descriptor $A$ for $L'$ 
	to describe formulas of depth $(c + o(1))\log \left(n^{d+o(1)}\right) = (c\cdot d + o(1))\log n$ for $L$.
	On input $x,b,n$, the descriptor machine $B$ for $L$  
	calls $A$ on $x, b, n^d$.
	If $A$ output a gate type or a constant, $B$ outputs the same information.
	If $A$ outputs a literal $x_i^*$, $B$ outputs $x_i^*$ if $i \le n$, and 1 otherwise.
\end{proof}

We need to be careful when applying the slowdown lemma to alternating classes:
the $i+1$-th computation stage takes as input a string of length $m = n^{a_i}$, not $n$.
Since the previous result only holds for $\NTIME[n^d], d \ge 1$, 
we need to take into account the edge case where the running time of the $i+1$-th
stage is sublinear in the size of its input.

We will now show that applying the Speedup rule once and
using exhaustive search to remove the logarithmic-size quantifier
yields an improved Slowdown rule for quantified classes.
We have the following:
\begin{corollary}[Extended Slowdown rule]\label{cor:extended_slowdown}
	If $\SAT{}\in \NDL{c}$, then for every $d > 1$, we have:
	\[\ldots (Q~n^{a}) (\neg Q~n^b) \NDL{d}
	\subseteq \ldots (Q~n^{a}) \NDL{c\cdot\max(d/2, a, b)}.\]
\end{corollary}
\begin{proof}
	Without loss of generality, assume that $Q = \forall$, and therefore $\neg Q = \exists$.
	In that case, for all $q, r > 0$, we have $(\neg Q~n^q) \NTIME[n^r] \subseteq \NTIME[n^{\max(q,r)}] $.
	Applying Corollary~\ref{cor:speedup} on the $\NDL{d}$ part and then using the 
	above observation leads to the following:
	\begin{flalign*}
	\ldots (Q~n^{a}) (\neg Q~n^b) \NDL{d} 
	&\subseteq \ldots (Q~n^{a}) (\neg Q~n^b) \NTIME[n^{d/2}]\\	
	&\subseteq \ldots (Q~n^{a}) \NTIME[n^{\max(d/2, d)}]	
	\end{flalign*}

	Now, consider the $\NTIME[n^{\max(d/2, b)}]$ language $L$
	that correspond to the last stage of the alternating 
	computation, and its corresponding nondeterministic machine $M$. 
	It takes as input a string of length $m = n^{a}$, and its runtime, as a function of $m$, 
	is $m^{\max(d/2, b)/a}$.
	We apply the Slowdown rule (Lemma~\ref{lemma:slowdown}) to $M$. There are two possible cases:
	\begin{itemize}
		\item if $\max(d/2, b)/a < 1$, then we have 
		$L \in \NTIME[m^{\max(d/2, b)/a}] \subseteq \NTIME[m]
		\subseteq \ND[c \log m] \subseteq \NDL{c \cdot a}$.
		Since  $\max(d/2, b) < a$, we have $a = \max(d/2, a, b)$ and therefore 
		$\NDL{c \cdot a} = \NDL{c\cdot\max(d/2, a, b)}$.
		\item otherwise, $\max(d/2, b) \ge a$, and applying the slowdown rule yields
		$L \in \NTIME[m^{\max(d/2, b)/a}] =  \NTIME[n^{\max(d/2, b)}] \subseteq \NDL{c \cdot \max(d/2, a, b)}$.
	\end{itemize}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%
\subsection{Structure of optimal proofs}
How to get the contradiction given the slowdown rule for $c < 4\cos(\pi /7)$.

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Improving Brent/Spira}
\todo[inline]{Prove this}
Useful starting point : Size-Depth Tradeoffs for Boolean Formulae (Bonet, Buss).

Remark: we must focus on the case when the input formula is a \NAND{} formula.
Otherwise, we prove a much stronger result.
Maybe prove the stronger result with a $2+\eps$ factor? Not very useful though.
If we can go below 2, we improve something.


%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Program to find optimal parameters}\label{app:ilp}
Improves over Williams (cite): no need to enumerate proof annotations.


Therein, Williams enumerates all possible proof skeletons of length $k$,
and tries to find to find the best set of parameters for each of them, using a linear program.
In this work, we propose a different approach, that proves to be more efficient in practice:
we use an Integer Linear program whose solution is
an optimal proof of length $k$
along with its parameters.

More precisely, we give an ILP $P_{c,k}$ which is feasible if and only if
there exists a proof annotation $p$ of length $k$\todo{it's not exactly that, it's $k$ up steps} along with parameters $x, r_0$
such that $p$ proves a contradiction under the assumption that $\SAT\in\NDL{c}$.
Moreover, its solution encodes $p,q,r_0$.
By using binary search over $c$, we can quickly get an approximation 
of the largest $c$ such that a proof of length $k$ can reach a contradiction under $H_c$\todo{define "hypothesis c"}, together with a proof for $c-\eps$.
Repeating the above steps for increasing values of $k$ shows that the sequence $c_k^*$\todo{define ?}
Table of $c_k^*$ for first few values of $k$?
Seems to show convergence for some value $c^\infty > 3.6$.

\todohere describe model + link to code?


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{biblio.bib}

\end{document}
